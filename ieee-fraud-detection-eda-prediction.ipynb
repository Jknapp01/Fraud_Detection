{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\n\n# Standard plotly imports\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.figure_factory as ff\n\ninit_notebook_mode(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gc\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-08T03:21:01.490729Z","iopub.execute_input":"2024-07-08T03:21:01.491152Z","iopub.status.idle":"2024-07-08T03:21:07.204056Z","shell.execute_reply.started":"2024-07-08T03:21:01.491120Z","shell.execute_reply":"2024-07-08T03:21:07.202833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Import datasets**","metadata":{}},{"cell_type":"markdown","source":"\n* functions to assist with memory reduction [source of function](https://www.kaggle.com/code/gemartin/load-data-reduce-memory-usage) and dataset representation inspired by and outlier calculations","metadata":{}},{"cell_type":"code","source":"def resumetable(df,operator,threshold):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values\n    summary['Missing Pct'] = (df.isnull().sum().values/len(df))*100\n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    \n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2), 2)\n\n    # Filter the summary dataframe based on the missing percentage threshold\n    summary = summary[eval(f\"summary['Missing Pct'] {operator} {threshold}\")]\n\n    # Sort the summary dataframe by 'Missing Pct' column in ascending order\n    summary = summary.sort_values(by='Missing Pct')\n\n    print(f\"Dataset Shape: {summary.shape}\")\n    return summary\n\n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df\n\n\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:07.206189Z","iopub.execute_input":"2024-07-08T03:21:07.206550Z","iopub.status.idle":"2024-07-08T03:21:07.229962Z","shell.execute_reply.started":"2024-07-08T03:21:07.206521Z","shell.execute_reply":"2024-07-08T03:21:07.228633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_id = import_data('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ndf_tr = import_data('/kaggle/input/ieee-fraud-detection/train_transaction.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:07.231325Z","iopub.execute_input":"2024-07-08T03:21:07.231706Z","iopub.status.idle":"2024-07-08T03:21:42.916598Z","shell.execute_reply.started":"2024-07-08T03:21:07.231672Z","shell.execute_reply":"2024-07-08T03:21:42.915401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our Goals:\n* To be able to predict which transactions are fraudulent \n* Understand the distribution of the data.(making sense of the data) \n* decide what to do with null values(mainly the features that have the nulls values)\n* check the relationship of the null value columns with the target(isFraud) \n\nIn getting to our goals there is a EDA which will be evolving as we try to improve our model performance.\ntherefore:\n1. EDA\n1. Pre-processing(since our data is Heavily imbalanced we must do something)\n2. Split data into (Training/Test)(70/30) respectively\n3.  ","metadata":{}},{"cell_type":"markdown","source":"# make sense of the data","metadata":{}},{"cell_type":"markdown","source":"functions to assist with data visualisation","metadata":{}},{"cell_type":"code","source":"\ndef hist_with_hue(df, col, hue):\n    plt.figure(figsize=(13.5, 6))\n\n    # Plotting the histogram with hue\n    plt.subplot(1, 2, 1)\n    sns.histplot(x=col, hue=hue, data=df, kde=True, palette='Set1')\n\n    # Configure the x-axis\n    ax = plt.gca()\n    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n    plt.xticks(rotation=90, ha='center')\n\n    plt.title(f\"Distribution by {col}\", fontweight=\"black\", size=14, pad=10)\n\n    # Plotting the boxplot with hue\n    plt.subplot(1, 2, 2)\n    sns.boxplot(x=hue, y=col, data=df, palette='Set2')\n\n    plt.title(f\"Distribution by {col} & {hue}\", fontweight=\"black\", size=14, pad=10)\n\n    plt.tight_layout()\n    plt.show()\n    \n    \n# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow, hue):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]]  # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = round((nCol + nGraphPerRow) / nGraphPerRow)\n    plt.figure(figsize=(6 * nGraphPerRow, 8 * nGraphRow), dpi=80, facecolor='w', edgecolor='k')\n\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        \n        if not np.issubdtype(columnDf.dtype, np.number):\n            sns.countplot(data=df, x=columnDf, hue=hue)\n        else:\n            sns.histplot(data=df, x=columnDf, hue=hue, kde=False, bins=30)\n        \n        plt.ylabel('Counts')\n        plt.xticks(rotation=90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    \n    plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:42.919179Z","iopub.execute_input":"2024-07-08T03:21:42.919567Z","iopub.status.idle":"2024-07-08T03:21:42.932370Z","shell.execute_reply.started":"2024-07-08T03:21:42.919528Z","shell.execute_reply":"2024-07-08T03:21:42.931240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df_tr['isFraud'].value_counts()[0]/len(df_tr) * 100,2), '% of the dataset')\nprint('Frauds', round(df_tr['isFraud'].value_counts()[1]/len(df_tr) * 100,2), '% of the dataset')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:42.933715Z","iopub.execute_input":"2024-07-08T03:21:42.934134Z","iopub.status.idle":"2024-07-08T03:21:42.968805Z","shell.execute_reply.started":"2024-07-08T03:21:42.934100Z","shell.execute_reply":"2024-07-08T03:21:42.967361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Merge Identity and transaction dataset**","metadata":{}},{"cell_type":"code","source":"train_df = df_tr.merge(df_id,how='left',left_index=True,right_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:42.970432Z","iopub.execute_input":"2024-07-08T03:21:42.970838Z","iopub.status.idle":"2024-07-08T03:21:43.833446Z","shell.execute_reply.started":"2024-07-08T03:21:42.970806Z","shell.execute_reply":"2024-07-08T03:21:43.832166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:43.834699Z","iopub.execute_input":"2024-07-08T03:21:43.835059Z","iopub.status.idle":"2024-07-08T03:21:43.843706Z","shell.execute_reply.started":"2024-07-08T03:21:43.835029Z","shell.execute_reply":"2024-07-08T03:21:43.842592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resumetable(train_df,\">=\",70) ","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:43.845179Z","iopub.execute_input":"2024-07-08T03:21:43.846027Z","iopub.status.idle":"2024-07-08T03:21:57.913508Z","shell.execute_reply.started":"2024-07-08T03:21:43.845984Z","shell.execute_reply":"2024-07-08T03:21:57.912334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:57.914862Z","iopub.execute_input":"2024-07-08T03:21:57.915223Z","iopub.status.idle":"2024-07-08T03:21:57.922563Z","shell.execute_reply.started":"2024-07-08T03:21:57.915192Z","shell.execute_reply":"2024-07-08T03:21:57.921350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***error*** bar(train_df,\"isFraud\",\"TransactionAmt\",\"Transactions Distribution\",\"Total Amount in Transaction Amt\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:48:17.749841Z","iopub.execute_input":"2024-07-07T20:48:17.750226Z","iopub.status.idle":"2024-07-07T20:48:18.276890Z","shell.execute_reply.started":"2024-07-07T20:48:17.750179Z","shell.execute_reply":"2024-07-07T20:48:18.275475Z"}}},{"cell_type":"markdown","source":"we have alot of dimensions, reducing these should take priority however preservation of information is to be considered\n* feature selection/importance","metadata":{}},{"cell_type":"markdown","source":"Categorical Features - Transaction\n\nProductCD\ncard1 - card6\naddr1, addr2\nP_emaildomain\nR_emaildomain\nM1 - M9\nCategorical Features - Identity\n\nDeviceType\nDeviceInfo\nid_12 - id_38\n","metadata":{}},{"cell_type":"markdown","source":"# Univariate Analysis","metadata":{}},{"cell_type":"code","source":"round((235 + 5-1) / 5)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:57.926668Z","iopub.execute_input":"2024-07-08T03:21:57.927138Z","iopub.status.idle":"2024-07-08T03:21:57.935922Z","shell.execute_reply.started":"2024-07-08T03:21:57.927101Z","shell.execute_reply":"2024-07-08T03:21:57.934644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plotPerColumnDistribution(train_df, 20, 5,'isFraud')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:10:20.662662Z","iopub.execute_input":"2024-07-08T03:10:20.663177Z","iopub.status.idle":"2024-07-08T03:10:29.630462Z","shell.execute_reply.started":"2024-07-08T03:10:20.663137Z","shell.execute_reply":"2024-07-08T03:10:29.628456Z"}}},{"cell_type":"markdown","source":"remove TransactionID_x, TransactionID_y, and TransactionDT","metadata":{}},{"cell_type":"code","source":"train_df.drop(columns=['TransactionID_x', 'TransactionDT'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:57.937559Z","iopub.execute_input":"2024-07-08T03:21:57.938027Z","iopub.status.idle":"2024-07-08T03:21:58.928453Z","shell.execute_reply.started":"2024-07-08T03:21:57.937986Z","shell.execute_reply":"2024-07-08T03:21:58.927278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categoric Columns are taken from Data tab of competition \ncategoric_columns = ['ProductCD',\n                     'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n                     'addr1', 'addr2',\n                     'P_emaildomain', 'R_emaildomain',\n                     'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n                     'DeviceType', 'DeviceInfo',\n                     'id_12', 'id_13', 'id_14', 'id_15', 'id_16','id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24',\n                     'id_25', 'id_26','id_27','id_28','id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34','id_35', 'id_36', 'id_37', 'id_38']\nnumeric_columns = []\n\nfor i in train_df.columns:\n    if i not in categoric_columns:\n        numeric_columns.append(i)\n\nprint('Numerical columns: ', numeric_columns)\nprint('Categoric columns: ', categoric_columns)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:58.929848Z","iopub.execute_input":"2024-07-08T03:21:58.930185Z","iopub.status.idle":"2024-07-08T03:21:58.939489Z","shell.execute_reply.started":"2024-07-08T03:21:58.930158Z","shell.execute_reply":"2024-07-08T03:21:58.938190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_columns.remove('isFraud')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:58.940953Z","iopub.execute_input":"2024-07-08T03:21:58.941426Z","iopub.status.idle":"2024-07-08T03:21:58.948988Z","shell.execute_reply.started":"2024-07-08T03:21:58.941393Z","shell.execute_reply":"2024-07-08T03:21:58.947727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[numeric_columns] = train_df[numeric_columns].astype('float64')","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:21:58.950883Z","iopub.execute_input":"2024-07-08T03:21:58.951364Z","iopub.status.idle":"2024-07-08T03:22:02.171356Z","shell.execute_reply.started":"2024-07-08T03:21:58.951320Z","shell.execute_reply":"2024-07-08T03:22:02.169862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalization and Encoding","metadata":{}},{"cell_type":"code","source":"# Initialize the scaler and encoder\nscaler = StandardScaler()\nencoder = LabelEncoder()\n    \n# Normalize numerical columns\ntrain_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n    \n# Encode categorical columns\nfor column in categoric_columns:\n    train_df[column] = encoder.fit_transform(train_df[column])","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:22:02.172721Z","iopub.execute_input":"2024-07-08T03:22:02.173108Z","iopub.status.idle":"2024-07-08T03:22:18.637704Z","shell.execute_reply.started":"2024-07-08T03:22:02.173077Z","shell.execute_reply":"2024-07-08T03:22:18.636334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:22:18.639295Z","iopub.execute_input":"2024-07-08T03:22:18.639744Z","iopub.status.idle":"2024-07-08T03:22:18.669218Z","shell.execute_reply.started":"2024-07-08T03:22:18.639702Z","shell.execute_reply":"2024-07-08T03:22:18.667723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handle Class Imbalance (isFraud)","metadata":{}},{"cell_type":"code","source":"(train_df['isFraud'] == 0).sum(), (train_df['isFraud'] == 1).sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:22:18.670794Z","iopub.execute_input":"2024-07-08T03:22:18.671219Z","iopub.status.idle":"2024-07-08T03:22:18.681375Z","shell.execute_reply.started":"2024-07-08T03:22:18.671187Z","shell.execute_reply":"2024-07-08T03:22:18.680046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import resample\n\n# Separate the majority and minority classes\nminority_class = train_df[train_df['isFraud'] == 1]\nmajority_class = train_df[train_df['isFraud'] == 0]\n\n# Upsample the minority class\nminority_upsampled = resample(minority_class, \n                              replace=True,     # sample with replacement\n                              n_samples=len(majority_class),  # match number in majority class\n                              random_state=42)  # reproducible results\n\n# Combine the upsampled minority class with the majority class\ntrain_df_balanced = pd.concat([minority_upsampled, majority_class])\n\n# Shuffle the resulting dataframe to mix the classes\ntrain_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Check the distribution of the classes\nprint(train_df_balanced['isFraud'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:22:27.563988Z","iopub.execute_input":"2024-07-08T03:22:27.564419Z","iopub.status.idle":"2024-07-08T03:22:48.871313Z","shell.execute_reply.started":"2024-07-08T03:22:27.564385Z","shell.execute_reply":"2024-07-08T03:22:48.869787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df_balanced","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:22:54.334874Z","iopub.execute_input":"2024-07-08T03:22:54.335285Z","iopub.status.idle":"2024-07-08T03:22:54.341097Z","shell.execute_reply.started":"2024-07-08T03:22:54.335255Z","shell.execute_reply":"2024-07-08T03:22:54.339812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['isFraud'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:12.980294Z","iopub.execute_input":"2024-07-08T03:23:12.981214Z","iopub.status.idle":"2024-07-08T03:23:12.996015Z","shell.execute_reply.started":"2024-07-08T03:23:12.981172Z","shell.execute_reply":"2024-07-08T03:23:12.994787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"before we visualise the numerical columns","metadata":{}},{"cell_type":"code","source":"# Calculate the percentage of null values in each column\nnull_percentage = train_df.isnull().mean() * 100","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:19.042955Z","iopub.execute_input":"2024-07-08T03:23:19.043427Z","iopub.status.idle":"2024-07-08T03:23:19.820931Z","shell.execute_reply.started":"2024-07-08T03:23:19.043392Z","shell.execute_reply":"2024-07-08T03:23:19.819489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a threshold for the null percentage\nthreshold = 60  # For example, consider columns with more than 60% null values\n# Filter the columns based on the null percentage threshold\nfiltered_columns = null_percentage[null_percentage > threshold].index\nprint(filtered_columns)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:20.572298Z","iopub.execute_input":"2024-07-08T03:23:20.572709Z","iopub.status.idle":"2024-07-08T03:23:20.580490Z","shell.execute_reply.started":"2024-07-08T03:23:20.572677Z","shell.execute_reply":"2024-07-08T03:23:20.579127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new DataFrame with the filtered columns\ntrain_df_filtered = train_df[filtered_columns]","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:21.184144Z","iopub.execute_input":"2024-07-08T03:23:21.184562Z","iopub.status.idle":"2024-07-08T03:23:21.685295Z","shell.execute_reply.started":"2024-07-08T03:23:21.184530Z","shell.execute_reply":"2024-07-08T03:23:21.683983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in filtered_columns:\n    if i in categoric_columns:\n        categoric_columns.remove(i)\n    else:\n        if i in numeric_columns:\n            numeric_columns.remove(i)\n            \nprint('Numerical columns: ', numeric_columns)\nprint('Categoric columns: ', categoric_columns)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:24.096679Z","iopub.execute_input":"2024-07-08T03:23:24.097456Z","iopub.status.idle":"2024-07-08T03:23:24.106361Z","shell.execute_reply.started":"2024-07-08T03:23:24.097417Z","shell.execute_reply":"2024-07-08T03:23:24.104722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of all numeric colums","metadata":{}},{"cell_type":"markdown","source":"**Statistical Analysis - Feature Importance**","metadata":{}},{"cell_type":"markdown","source":"1. Performing ANOVA Test to Analyze the Numerical Features Importance in Fraud detection.","metadata":{}},{"cell_type":"code","source":"f_scores = {}\np_values = {}\n\nfor column in numeric_columns[:20]:\n    f_score, p_value = stats.f_oneway(train_df[column], train_df[\"isFraud\"])\n    \n    f_scores[column] = f_score\n    p_values[column] = p_value","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:27.090410Z","iopub.execute_input":"2024-07-08T03:23:27.090844Z","iopub.status.idle":"2024-07-08T03:23:27.602806Z","shell.execute_reply.started":"2024-07-08T03:23:27.090810Z","shell.execute_reply":"2024-07-08T03:23:27.601541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming f_scores is a dictionary with keys and values\nkeys = list(f_scores.keys())\nvalues = list(f_scores.values())\n\nplt.figure(figsize=(15, 6))\nsns.barplot(x=keys, y=values)\nplt.title(\"Anova-Test F_scores Comparison\", fontweight=\"black\", size=16, pad=15)\nplt.xticks(rotation=90)\n\n# Adding annotations\nfor index, value in enumerate(values):\n    if not np.isnan(value):  # Check if the value is not NaN\n        plt.text(index, value, f'{int(value)}', ha='center', va='bottom', size=14)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:39.379447Z","iopub.execute_input":"2024-07-08T03:23:39.379883Z","iopub.status.idle":"2024-07-08T03:23:39.840729Z","shell.execute_reply.started":"2024-07-08T03:23:39.379850Z","shell.execute_reply":"2024-07-08T03:23:39.839312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(columns='isFraud')\ny = train_df['isFraud']","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:46.740833Z","iopub.execute_input":"2024-07-08T03:23:46.741318Z","iopub.status.idle":"2024-07-08T03:23:47.929335Z","shell.execute_reply.started":"2024-07-08T03:23:46.741282Z","shell.execute_reply":"2024-07-08T03:23:47.928187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:48.618365Z","iopub.execute_input":"2024-07-08T03:23:48.618910Z","iopub.status.idle":"2024-07-08T03:23:48.628845Z","shell.execute_reply.started":"2024-07-08T03:23:48.618865Z","shell.execute_reply":"2024-07-08T03:23:48.627406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:48.972309Z","iopub.execute_input":"2024-07-08T03:23:48.972748Z","iopub.status.idle":"2024-07-08T03:23:48.980705Z","shell.execute_reply.started":"2024-07-08T03:23:48.972711Z","shell.execute_reply":"2024-07-08T03:23:48.979322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:50.170571Z","iopub.execute_input":"2024-07-08T03:23:50.170998Z","iopub.status.idle":"2024-07-08T03:23:56.095030Z","shell.execute_reply.started":"2024-07-08T03:23:50.170964Z","shell.execute_reply":"2024-07-08T03:23:56.093788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Test","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n# Instantiate XGBoost classifier\nmodel = XGBClassifier(random_state=42)\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:23:56.096972Z","iopub.execute_input":"2024-07-08T03:23:56.097318Z","iopub.status.idle":"2024-07-08T03:25:08.296573Z","shell.execute_reply.started":"2024-07-08T03:23:56.097289Z","shell.execute_reply":"2024-07-08T03:25:08.295463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:25:08.298370Z","iopub.execute_input":"2024-07-08T03:25:08.298710Z","iopub.status.idle":"2024-07-08T03:25:09.882264Z","shell.execute_reply.started":"2024-07-08T03:25:08.298682Z","shell.execute_reply":"2024-07-08T03:25:09.881037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict probabilities for the test set\ny_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of class 1 (positive class)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:25:09.884983Z","iopub.execute_input":"2024-07-08T03:25:09.885886Z","iopub.status.idle":"2024-07-08T03:25:10.972393Z","shell.execute_reply.started":"2024-07-08T03:25:09.885837Z","shell.execute_reply":"2024-07-08T03:25:10.970846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# Calculate AUC-ROC score\nauc_roc = roc_auc_score(y_test, y_pred_proba)\nprint(f\"AUC-ROC Score: {auc_roc:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-08T03:25:10.974228Z","iopub.execute_input":"2024-07-08T03:25:10.974776Z","iopub.status.idle":"2024-07-08T03:25:11.071867Z","shell.execute_reply.started":"2024-07-08T03:25:10.974716Z","shell.execute_reply":"2024-07-08T03:25:11.070652Z"},"trusted":true},"execution_count":null,"outputs":[]}]}